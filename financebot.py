# ç¦ç”Ÿæ— é‡å¤©å°Š
from openai import OpenAI
import feedparser
import requests
from newspaper import Article
from datetime import datetime
import time
import pytz
import os
# OpenAI API Key
# openai_api_key = os.getenv("OPENAI_API_KEY")
# ä»ç¯å¢ƒå˜é‡è·å– Serveré…± SendKeys
server_chan_keys_env = os.getenv("SERVER_CHAN_KEYS")
if not server_chan_keys_env:
    raise ValueError("ç¯å¢ƒå˜é‡ SERVER_CHAN_KEYS æœªè®¾ç½®ï¼Œè¯·åœ¨Github Actionsä¸­è®¾ç½®æ­¤å˜é‡ï¼")
server_chan_keys = server_chan_keys_env.split(",")

# openai_client = OpenAI(api_key=openai_api_key, base_url="https://api.deepseek.com/v1")
# è·å–DashScope API Key
dashscope_api_key = os.getenv("DASHSCOPE_API_KEY")
if not dashscope_api_key:
    raise ValueError("ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY æœªè®¾ç½®ï¼Œè¯·åœ¨Github Actionsä¸­è®¾ç½®æ­¤å˜é‡ï¼")

openai_client = OpenAI(
    api_key=dashscope_api_key,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# RSSæºåœ°å€åˆ—è¡¨
rss_feeds = {
    "ğŸ“ˆ å¯Œé€”ç‰›ç‰›": {
        "å¯Œé€”ç‰›ç‰›å¸‚åœºèµ„è®¯": {
            "url": "https://news.futunn.com/news-site-api/main/get-market-list?size=15",
            "type": "json"
        }
    },
    "ğŸ“Š æ ¼éš†æ±‡": {
        "æ ¼éš†æ±‡è´¢ç»": {
            "url": "https://www.gelonghui.com/api/live-channels/all/lives/v4?category=all&limit=15",
            "type": "json2"
        }
    },
    "ğŸ’°æ™ºé€šè´¢ç»": {
        "æ™ºé€šè´¢ç»": {
            "url": "https://mapi.zhitongcaijing.com/news/list.html?mode=history&access_token=&category_id=index_shouye&category_key=&language=zh-cn&last_time=&page=1&tradition_chinese=0",
            "type": "json3"
        }
    },
    "ğŸŒ é‡‘è‰²è´¢ç»": {
            "é‡‘è‰²è´¢ç»å¿«è®¯": {
                "url": "https://api.jinse.cn/noah/v2/lives?limit=20&reading=false&source=web&flag=up&id=0&category=0",
                "type": "json4"
            }
      },
    "ğŸ’²åå°”è¡—è§é—»":{
        "åå°”è¡—è§é—»":"https://dedicated.wallstreetcn.com/rss.xml",
    },
     "å½­åšç¤¾":{
          "å½­åšç¤¾":"https://bloombergnew.buzzing.cc/feed.xml"
     },
    "ğŸ’» 36æ°ª":{
        "36æ°ª":"https://36kr.com/feed",
    },
    "è´¢è”ç¤¾":{
          "è´¢è”ç¤¾å¤´æ¡":' https://rsshub.app/cls/telegraph/red',
          "è´¢è”ç¤¾çƒ­é—¨":' https://rsshub.app/cls/hot'
    },
     "ğŸ‡ºğŸ‡¸ ç¾å›½ç»æµ": {
            "åå°”è¡—æ—¥æŠ¥ - ç»æµ":"https://cn.wsj.com/zh-hans/rss",
            "è”åˆæ—©æŠ¥":"https://plink.anyfeeder.com/zaobao/realtime/world",
            "çº½çº¦æ—¶æŠ¥":"https://plink.anyfeeder.com/nytimes/dual",
            "åå°”è¡—æ—¥æŠ¥ - å¸‚åœº":"https://feeds.content.dowjones.io/public/rss/RSSMarketsMain",
            "é›…è™è´¢ç»":'https://yahoo.buzzing.cc/feed.xml',
#             "MarketWatchç¾è‚¡": "https://www.marketwatch.com/rss/topstories",
#             "ZeroHedgeåå°”è¡—æ–°é—»": "https://feeds.feedburner.com/zerohedge/feed",
#             "ETF Trends": "https://www.etftrends.com/feed/"
    },
    "ğŸ‡¨ğŸ‡³ ä¸­å›½ç»æµ": {
        "é¦™æ¸¯ç¶“æ¿Ÿæ—¥å ±":"https://www.hket.com/rss/china",
        "ä¸œæ–¹è´¢å¯Œ":"http://rss.eastmoney.com/rss_partener.xml",
        "ç™¾åº¦è‚¡ç¥¨ç„¦ç‚¹":"http://news.baidu.com/n?cmd=1&class=stock&tn=rss&sub=0",
        "ä¸­æ–°ç½‘":"https://www.chinanews.com.cn/rss/finance.xml",
        "åŒèŠ±é¡º":"https://rsshub.app/10jqka/realtimenews"
    },

#     "ğŸŒ ä¸–ç•Œç»æµ": {
#         "åå°”è¡—æ—¥æŠ¥ - ç»æµ":"https://feeds.content.dowjones.io/public/rss/socialeconomyfeed",
#         "BBCå…¨çƒç»æµ": "http://feeds.bbci.co.uk/news/business/rss.xml",
#     },
}

# è·å–åŒ—äº¬æ—¶é—´
def today_date():
    return datetime.now(pytz.timezone("Asia/Shanghai")).date()

# çˆ¬å–ç½‘é¡µæ­£æ–‡ (ç”¨äº AI åˆ†æï¼Œä½†ä¸å±•ç¤º)
def fetch_article_text(url):
    try:
        print(f"ğŸ“° æ­£åœ¨çˆ¬å–æ–‡ç« å†…å®¹: {url}")
        article = Article(url)
        article.download()
        article.parse()
        text = article.text[:1500]  # é™åˆ¶é•¿åº¦ï¼Œé˜²æ­¢è¶…å‡º API è¾“å…¥é™åˆ¶
        if not text:
            print(f"âš ï¸ æ–‡ç« å†…å®¹ä¸ºç©º: {url}")
        return text
    except Exception as e:
        print(f"âŒ æ–‡ç« çˆ¬å–å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}")
        return "ï¼ˆæœªèƒ½è·å–æ–‡ç« æ­£æ–‡ï¼‰"

# æ·»åŠ  User-Agent å¤´
def fetch_feed_with_headers(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    return feedparser.parse(url, request_headers=headers)

# è‡ªåŠ¨é‡è¯•è·å– RSS
def fetch_feed_with_retry(url, retries=3, delay=5):
    for i in range(retries):
        try:
            feed = fetch_feed_with_headers(url)
            if feed and hasattr(feed, 'entries') and len(feed.entries) > 0:
                return feed
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {i+1} æ¬¡è¯·æ±‚ {url} å¤±è´¥: {e}")
            time.sleep(delay)
    print(f"âŒ è·³è¿‡ {url}, å°è¯• {retries} æ¬¡åä»å¤±è´¥ã€‚")
    return None

# è·å–å¯Œé€”ç‰›ç‰›JSONæ¥å£å†…å®¹
def fetch_json_articles(url, max_articles=20):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        data = response.json()
        articles = []
        analysis_text = ""

        # è§£æJSONæ•°æ®è·å–æ–‡ç« åˆ—è¡¨
        news_list = data.get('data', {}).get('list', [])

        for item in news_list[:max_articles]:
            title = item.get('title', 'æ— æ ‡é¢˜')
            abstract = item.get('abstract', '')
            link = item.get('url', '')

            if not link:
                continue

            articles.append(f"- [{title}]({link})")

            # ä½¿ç”¨æ ‡é¢˜å’Œæ‘˜è¦ä½œä¸ºAIåˆ†æçš„ä¸»è¦å†…å®¹
            content_for_analysis = f"{title}\n{abstract}"
            analysis_text += f"ã€{title}ã€‘\n{content_for_analysis}\n\n"

        return articles, analysis_text

    except Exception as e:
        print(f"âŒ å¯Œé€”ç‰›ç‰›æ¥å£è·å–å¤±è´¥: {e}")
        return [], ""

# è·å–æ ¼éš†æ±‡JSONæ¥å£å†…å®¹
def fetch_gelonghui_articles(url, max_articles=20):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        data = response.json()
        articles = []
        analysis_text = ""

        # è§£æJSONæ•°æ®è·å–æ–‡ç« åˆ—è¡¨
        news_list = data.get('result', [])

        for item in news_list[:max_articles]:
            title = item.get('title', '').strip()
            content = item.get('content', '').strip()
            link = item.get('route', '').strip()

            # å¦‚æœæ ‡é¢˜ã€å†…å®¹ã€é“¾æ¥éƒ½ä¸ºç©ºï¼Œåˆ™è·³è¿‡
            if not title or not content or not link:
                continue

            articles.append(f"- [{title}]({link})")

            # ä½¿ç”¨æ ‡é¢˜å’Œå†…å®¹ä½œä¸ºAIåˆ†æçš„ä¸»è¦å†…å®¹
            content_for_analysis = f"{title}\n{content}"
            analysis_text += f"ã€{title}ã€‘\n{content_for_analysis}\n\n"

        return articles, analysis_text

    except Exception as e:
        print(f"âŒ æ ¼éš†æ±‡æ¥å£è·å–å¤±è´¥: {e}")
        return [], ""


# è·å–æ™ºé€šè´¢ç»JSONæ¥å£å†…å®¹
def fetch_zhitongcaijing_articles(url, max_articles=20):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        data = response.json()
        articles = []
        analysis_text = ""

        # è§£æJSONæ•°æ®è·å–æ–‡ç« åˆ—è¡¨
        news_list = data.get('data', {}).get('list', [])

        for item in news_list[:max_articles]:
            title = item.get('title', 'æ— æ ‡é¢˜')
            digest = item.get('digest', '')
            url_path = item.get('url', '')

            # æ„é€ å®Œæ•´é“¾æ¥
            if url_path:
                link = f"https://mapi.zhitongcaijing.com{url_path}"
            else:
                link = ""

            if not link:
                continue

            articles.append(f"- [{title}]({link})")

            # ä½¿ç”¨æ ‡é¢˜å’Œæ‘˜è¦ä½œä¸ºAIåˆ†æçš„ä¸»è¦å†…å®¹
            content_for_analysis = f"{title}\n{digest}"
            analysis_text += f"ã€{title}ã€‘\n{content_for_analysis}\n\n"

        return articles, analysis_text

    except Exception as e:
        print(f"âŒ æ™ºé€šè´¢ç»æ¥å£è·å–å¤±è´¥: {e}")
        return [], ""

# è·å–é‡‘è‰²è´¢ç»JSONæ¥å£å†…å®¹
def fetch_jinse_articles(url, max_articles=20):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/json',
            'Referer': 'https://www.jinse.cn/'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        data = response.json()
        articles = []
        analysis_text = ""

        # è§£æJSONæ•°æ®è·å–æ–‡ç« åˆ—è¡¨
        live_list = []
        if 'list' in data:
            # æŒ‰æ—¥æœŸåˆ†ç»„çš„æ•°æ®ç»“æ„
            for date_group in data['list']:
                if 'lives' in date_group:
                    live_list.extend(date_group['lives'])
        elif 'lives' in data:
            # ç›´æ¥çš„livesåˆ—è¡¨
            live_list = data['lives']

        for item in live_list[:max_articles]:
            content = item.get('content', 'æ— å†…å®¹')
            link = item.get('link', '')
            id = item.get('id', '')

            if not content:
                continue

            # å¦‚æœæ²¡æœ‰é“¾æ¥ï¼Œæ„é€ ä¸€ä¸ª
            if not link and id:
                link = f"https://www.jinse.cn/news/detail/{id}.html"

            # æå–æ ‡é¢˜ï¼ˆä½¿ç”¨å†…å®¹çš„å‰30ä¸ªå­—ç¬¦ä½œä¸ºæ ‡é¢˜ï¼‰
            title = content[:30] + "..." if len(content) > 30 else content

            articles.append(f"- [{title}]({link})")

            # ä½¿ç”¨å†…å®¹ä½œä¸ºAIåˆ†æçš„ä¸»è¦å†…å®¹
            content_for_analysis = content
            analysis_text += f"ã€{title}ã€‘\n{content_for_analysis}\n\n"

        return articles, analysis_text

    except Exception as e:
        print(f"âŒ é‡‘è‰²è´¢ç»æ¥å£è·å–å¤±è´¥: {e}")
        return [], ""

# è·å–RSSå†…å®¹ï¼ˆçˆ¬å–æ­£æ–‡ä½†ä¸å±•ç¤ºï¼‰
def fetch_rss_articles(rss_feeds, max_articles=10):
    news_data = {}
    analysis_text = ""  # ç”¨äºAIåˆ†æçš„æ­£æ–‡å†…å®¹

    for category, sources in rss_feeds.items():
        category_content = ""
        for source, source_info in sources.items():
            # åˆ¤æ–­æ˜¯RSSæºè¿˜æ˜¯JSONæº
            if isinstance(source_info, dict) and source_info.get('type') == 'json':
                # å¤„ç†å¯Œé€”ç‰›ç‰›JSONæ¥å£æº
                print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ JSON æ¥å£: {source_info['url']}")
                articles, analysis = fetch_json_articles(source_info['url'], 15)  # å¯Œé€”ç‰›ç‰›å›ºå®šè·å–15æ¡
                analysis_text += analysis
                articles_content = ""
                if articles:
                    articles_content = f"### {source}\n" + "\n".join(articles) + "\n\n"
            elif isinstance(source_info, dict) and source_info.get('type') == 'json2':
                # å¤„ç†æ ¼éš†æ±‡JSONæ¥å£æº
                print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ JSON æ¥å£: {source_info['url']}")
                articles, analysis = fetch_gelonghui_articles(source_info['url'], 15)  # æ ¼éš†æ±‡å›ºå®šè·å–15æ¡
                analysis_text += analysis
                articles_content = ""
                if articles:
                    articles_content = f"### {source}\n" + "\n".join(articles) + "\n\n"
            elif isinstance(source_info, dict) and source_info.get('type') == 'json3':
                # å¤„ç†æ™ºé€šè´¢ç»JSONæ¥å£æº
                print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ JSON æ¥å£: {source_info['url']}")
                articles, analysis = fetch_zhitongcaijing_articles(source_info['url'], 10)  # æ™ºé€šè´¢ç»å›ºå®šè·å–10æ¡
                analysis_text += analysis
                articles_content = ""
                if articles:
                    articles_content = f"### {source}\n" + "\n".join(articles) + "\n\n"
            elif isinstance(source_info, dict) and source_info.get('type') == 'json4':
                # å¤„ç†é‡‘è‰²è´¢ç»JSONæ¥å£æº
                print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ JSON æ¥å£: {source_info['url']}")
                articles, analysis = fetch_jinse_articles(source_info['url'], 20)  # é‡‘è‰²è´¢ç»å›ºå®šè·å–20æ¡
                analysis_text += analysis
                articles_content = ""
                if articles:
                    articles_content = f"### {source}\n" + "\n".join(articles) + "\n\n"
            else:
                # å¤„ç†åŸæœ‰çš„RSSæº
                url = source_info
                print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ RSS æº: {url}")

                # ç‰¹æ®Šå¤„ç†åå°”è¡—è§é—»ï¼Œè·å–10æ¡æ–°é—»
                if source == "åå°”è¡—è§é—»":
                    feed_max_articles = 10
                else:
                    feed_max_articles = max_articles

                feed = fetch_feed_with_retry(url)
                if not feed:
                    print(f"âš ï¸ æ— æ³•è·å– {source} çš„ RSS æ•°æ®")
                    continue
                print(f"âœ… {source} RSS è·å–æˆåŠŸï¼Œå…± {len(feed.entries)} æ¡æ–°é—»")

                articles = []  # æ¯ä¸ªsourceéƒ½éœ€è¦é‡æ–°åˆå§‹åŒ–åˆ—è¡¨
                for entry in feed.entries[:feed_max_articles]:
                    title = entry.get('title', 'æ— æ ‡é¢˜')
                    link = entry.get('link', '') or entry.get('guid', '')
                    if not link:
                        print(f"âš ï¸ {source} çš„æ–°é—» '{title}' æ²¡æœ‰é“¾æ¥ï¼Œè·³è¿‡")
                        continue

                    # çˆ¬å–æ­£æ–‡ç”¨äºåˆ†æï¼ˆä¸å±•ç¤ºï¼‰
                    article_text = fetch_article_text(link)
                    analysis_text += f"ã€{title}ã€‘\n{article_text}\n\n"

                    print(f"ğŸ”¹ {source} - {title} è·å–æˆåŠŸ")
                    articles.append(f"- [{title}]({link})")

                articles_content = ""
                if articles:
                    articles_content = f"### {source}\n" + "\n".join(articles) + "\n\n"

            category_content += articles_content

        news_data[category] = category_content

    return news_data, analysis_text

# AI ç”Ÿæˆå†…å®¹æ‘˜è¦ï¼ˆåŸºäºçˆ¬å–çš„æ­£æ–‡ï¼‰
def summarize(text):
    # è·å–å½“å‰åŒ—äº¬æ—¶é—´
    current_date = datetime.now(pytz.timezone("Asia/Shanghai")).strftime("%Y-%m-%d")
    current_weekday = datetime.now(pytz.timezone("Asia/Shanghai")).strftime("%A")

    completion = openai_client.chat.completions.create(
        model="qwen-max",  # æˆ–å…¶ä»–ä½ é€‰æ‹©çš„æ¨¡å‹
        messages=[
           {"role": "system", "content": f"""
           ä½ æ˜¯ä¸€åä¸“ä¸šçš„è´¢ç»æ–°é—»åˆ†æå¸ˆå’ŒæŠ€æœ¯åˆ†æä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ–°é—»å†…å®¹ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å®Œæˆä»»åŠ¡ï¼š
           ä»Šå¤©æ˜¯{current_date}ï¼Œæ˜ŸæœŸ{current_weekday}ã€‚è¯·åŸºäºä»Šæ—¥çš„å¸‚åœºæƒ…å†µæ¥åˆ†æä»¥ä¸‹æ–°é—»å†…å®¹ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å®Œæˆä»»åŠ¡ï¼š

           1. æå–æ–°é—»ä¸­æ¶‰åŠçš„ä¸»è¦è¡Œä¸šå’Œä¸»é¢˜ï¼Œæ‰¾å‡ºè¿‘1å¤©æ¶¨å¹…æœ€é«˜çš„3ä¸ªè¡Œä¸šæˆ–ä¸»é¢˜ï¼Œä»¥åŠè¿‘3å¤©æ¶¨å¹…è¾ƒé«˜ä¸”æ­¤å‰2å‘¨è¡¨ç°å¹³æ·¡çš„3ä¸ªè¡Œä¸š/ä¸»é¢˜ã€‚ï¼ˆå¦‚æ–°é—»æœªæä¾›å…·ä½“æ¶¨å¹…ï¼Œè¯·ç»“åˆæè¿°å’Œå¸‚åœºæƒ…ç»ªæ¨æµ‹çƒ­ç‚¹ï¼‰

           2. é’ˆå¯¹æ¯æ¡é‡è¦æ–°é—»ï¼Œåˆ†æå…¶å¯¹å¸‚åœºçš„å½±å“ï¼š
              - åˆ¤æ–­è¯¥æ–°é—»æ˜¯åˆ©å¥½è¿˜æ˜¯åˆ©ç©º
              - æŒ‡å‡ºæœ€å—å½±å“çš„ä¸ªè‚¡æˆ–æ¿å—ï¼ˆ2-3ä¸ªï¼‰
              - ç®€è¦è¯´æ˜å½±å“åŸå› 

           3. é’ˆå¯¹æ¯ä¸ªçƒ­ç‚¹è¡Œä¸š/ä¸»é¢˜ï¼Œè¾“å‡ºï¼š
              - å‚¬åŒ–å‰‚ï¼šåˆ†æè¿‘æœŸä¸Šæ¶¨çš„å¯èƒ½åŸå› ï¼ˆæ”¿ç­–ã€æ•°æ®ã€äº‹ä»¶ã€æƒ…ç»ªç­‰ï¼‰ã€‚
              - å¤ç›˜ï¼šæ¢³ç†è¿‡å»3ä¸ªæœˆè¯¥è¡Œä¸š/ä¸»é¢˜çš„æ ¸å¿ƒé€»è¾‘ã€å…³é”®åŠ¨æ€ä¸é˜¶æ®µæ€§èµ°åŠ¿ã€‚
              - å±•æœ›ï¼šåˆ¤æ–­è¯¥çƒ­ç‚¹æ˜¯çŸ­æœŸç‚’ä½œè¿˜æ˜¯æœ‰æŒç»­è¡Œæƒ…æ½œåŠ›ã€‚
              - æ ¸å¿ƒä¸ªè‚¡/ETFï¼šå¦‚æœæ˜¯æ¿å—æ¦‚å¿µï¼Œæ‰¾å‡ºè¯¥æ¿å—çš„é¾™å¤´è‚¡2-3åªï¼›å¦‚æœæ˜¯å…·ä½“å…¬å¸æ–°é—»ï¼Œç›´æ¥æŒ‡å‡ºç›¸å…³å…¬å¸ã€‚åˆ—å‡ºå¯¹åº”çš„è‚¡ç¥¨ä»£ç å’Œåç§°ã€‚
              - æŠ€æœ¯åˆ†æï¼šåŸºäºæ–°é—»ä¸­æåˆ°çš„è¶‹åŠ¿ã€æˆäº¤é‡ã€çªç ´ã€å›è°ƒç­‰æè¿°ï¼Œè¿›è¡ŒæŠ€æœ¯é¢è§£è¯»ï¼ˆæ³¨æ„ï¼šä¸è¦å‡è®¾ä»»ä½•å…·ä½“çš„è‚¡ä»·æ•°æ®ï¼‰ã€‚
              - ä¹°ç‚¹æç¤ºï¼šç»“åˆæ–°é—»ä¸­çš„æè¿°ï¼Œç»™å‡ºå¯èƒ½çš„æ“ä½œå»ºè®®ï¼ˆä¾‹å¦‚ï¼šâ€œè‹¥å‡ºç°æ”¾é‡çªç ´å¯å…³æ³¨å…¥åœºæœºä¼šâ€ï¼‰ã€‚
              - é‡è¦æé†’ï¼šè¯·ä¸è¦æä¾›ä»»ä½•å…·ä½“çš„è‚¡ä»·æ•°å€¼æˆ–å®æ—¶è¡Œæƒ…æ•°æ®ã€‚

           4. å°†ä»¥ä¸Šåˆ†ææ•´åˆä¸ºä¸€ç¯‡1500å­—ä»¥å†…çš„è´¢ç»çƒ­ç‚¹æ‘˜è¦ï¼Œé€»è¾‘æ¸…æ™°ã€é‡ç‚¹çªå‡ºï¼Œé€‚åˆä¸“ä¸šæŠ•èµ„è€…é˜…è¯»ã€‚
           5. æœ€åç»™å‡ºä¸€ä¸ª"ä»Šæ—¥é‡ç‚¹å…³æ³¨"å°ç»“ï¼Œåˆ—å‡º3-5ä¸ªæœ€å€¼å¾—å…³æ³¨çš„ä¸ªè‚¡åŠå…¶ç®€è¦ç†ç”±å’Œæ“ä½œå»ºè®®ã€‚
           6. é‡è¦ï¼šæ‰€æœ‰ä»·æ ¼ç›¸å…³åˆ†æåº”åŸºäºæ–°é—»æè¿°è€Œéå…·ä½“æ•°å€¼ï¼Œä¸è¦ç¼–é€ ä¸å­˜åœ¨çš„ä»·æ ¼æ•°æ®ã€‚

             æ³¨æ„äº‹é¡¹ï¼š
             - ä¸ªè‚¡æ¨èéœ€åŸºäºæ–°é—»å†…å®¹å’Œåˆç†æ¨æ–­ï¼Œä¸è¦éšæ„ç¼–é€ ä¸å­˜åœ¨çš„å…³è”
             - æŠ€æœ¯åˆ†æè¦ç»“åˆè¶‹åŠ¿ã€æ”¯æ’‘é˜»åŠ›ä½ã€æˆäº¤é‡ç­‰è¦ç´ ï¼Œå¹¶åŸºäº{current_date}å½“å¤©çš„è‚¡ä»·æ•°æ®
             - ä¹°ç‚¹æç¤ºè¦å…·ä½“ï¼ŒåŒ…æ‹¬ä»·æ ¼åŒºé—´å’Œé£é™©æ§åˆ¶å»ºè®®
             - æ‰€æœ‰åˆ†æå»ºè®®ä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®
             - ä¸è¦æä¾›ä»»ä½•å…·ä½“çš„å®æ—¶è‚¡ä»·æ•°æ®
             """},
            {"role": "user", "content": text}
        ]
    )
    return completion.choices[0].message.content.strip()

# å‘é€å¾®ä¿¡æ¨é€
def send_to_wechat(title, content):
    for key in server_chan_keys:
        url = f"https://sctapi.ftqq.com/{key}.send"
        data = {"title": title, "desp": content}
        response = requests.post(url, data=data, timeout=10)
        if response.ok:
            print(f"âœ… æ¨é€æˆåŠŸ: {key}")
        else:
            print(f"âŒ æ¨é€å¤±è´¥: {key}, å“åº”ï¼š{response.text}")


if __name__ == "__main__":
    today_str = today_date().strftime("%Y-%m-%d")

    # æ¯ä¸ªç½‘ç«™è·å–æœ€å¤š 5 ç¯‡æ–‡ç« ï¼ˆå¯Œé€”ç‰›ç‰›ã€æ ¼éš†æ±‡ã€æ™ºé€šè´¢ç»å’Œåå°”è¡—è§é—»é™¤å¤–ï¼‰
    articles_data, analysis_text = fetch_rss_articles(rss_feeds, max_articles=10)

    # AIç”Ÿæˆæ‘˜è¦
    summary = summarize(analysis_text)

    # ç”Ÿæˆä»…å±•ç¤ºæ ‡é¢˜å’Œé“¾æ¥çš„æœ€ç»ˆæ¶ˆæ¯
    final_summary = f"ğŸ“… **{today_str} è´¢ç»æ–°é—»æ‘˜è¦**\n\nâœï¸ **ä»Šæ—¥åˆ†ææ€»ç»“ï¼š**\n{summary}\n\n---\n\n"
    for category, content in articles_data.items():
        if content.strip():
            final_summary += f"## {category}\n{content}\n\n"

    # æ¨é€åˆ°å¤šä¸ªserveré…±key
    send_to_wechat(title=f"ğŸ“Œ {today_str} è´¢ç»æ–°é—»æ‘˜è¦", content=final_summary)
