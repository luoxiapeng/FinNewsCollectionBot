# ç¦ç”Ÿæ— é‡å¤©å°Š
from openai import OpenAI
import feedparser
import requests
from newspaper import Article
from datetime import datetime
import time
import pytz
import os
# OpenAI API Key
# openai_api_key = os.getenv("OPENAI_API_KEY")
# ä»ç¯å¢ƒå˜é‡è·å– Serveré…± SendKeys
server_chan_keys_env = os.getenv("SERVER_CHAN_KEYS")
if not server_chan_keys_env:
    raise ValueError("ç¯å¢ƒå˜é‡ SERVER_CHAN_KEYS æœªè®¾ç½®ï¼Œè¯·åœ¨Github Actionsä¸­è®¾ç½®æ­¤å˜é‡ï¼")
server_chan_keys = server_chan_keys_env.split(",")

# openai_client = OpenAI(api_key=openai_api_key, base_url="https://api.deepseek.com/v1")
# è·å–DashScope API Key
dashscope_api_key = os.getenv("DASHSCOPE_API_KEY")
if not dashscope_api_key:
    raise ValueError("ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY æœªè®¾ç½®ï¼Œè¯·åœ¨Github Actionsä¸­è®¾ç½®æ­¤å˜é‡ï¼")

openai_client = OpenAI(
    api_key=dashscope_api_key,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# RSSæºåœ°å€åˆ—è¡¨
rss_feeds = {
    "ğŸ“ˆ å¯Œé€”ç‰›ç‰›": {
        "å¯Œé€”ç‰›ç‰›å¸‚åœºèµ„è®¯": {
            "url": "https://news.futunn.com/news-site-api/main/get-market-list?size=15",
            "type": "json"
        }
    },
    "ğŸ“Š æ ¼éš†æ±‡": {
        "æ ¼éš†æ±‡è´¢ç»": {
            "url": "https://www.gelonghui.com/api/live-channels/all/lives/v4?category=all&limit=15",
            "type": "json2"
        }
    },
    "ğŸ’°æ™ºé€šè´¢ç»": {
        "æ™ºé€šè´¢ç»": {
            "url": "https://mapi.zhitongcaijing.com/news/list.html?mode=history&access_token=&category_id=index_shouye&category_key=&language=zh-cn&last_time=&page=1&tradition_chinese=0",
            "type": "json3"
        }
    },
    "ğŸŒ é‡‘è‰²è´¢ç»": {
            "é‡‘è‰²è´¢ç»å¿«è®¯": {
                "url": "https://api.jinse.cn/noah/v2/lives?limit=20&reading=false&source=web&flag=up&id=0&category=0",
                "type": "json4"
            }
      },
    "ğŸ’²åå°”è¡—è§é—»":{
        "åå°”è¡—è§é—»":"https://dedicated.wallstreetcn.com/rss.xml",
    },
     "å½­åšç¤¾":{
          "å½­åšç¤¾":"https://bloombergnew.buzzing.cc/feed.xml"
     },
    "ğŸ’» 36æ°ª":{
        "36æ°ª":"https://36kr.com/feed",
    },
    "è´¢è”ç¤¾":{
          "è´¢è”ç¤¾å¤´æ¡":' https://rsshub.app/cls/telegraph/red',
          "è´¢è”ç¤¾çƒ­é—¨":' https://rsshub.app/cls/hot'
    },
     "ğŸ‡ºğŸ‡¸ ç¾å›½ç»æµ": {
            "åå°”è¡—æ—¥æŠ¥ - ç»æµ":"https://cn.wsj.com/zh-hans/rss",
            "è”åˆæ—©æŠ¥":"https://plink.anyfeeder.com/zaobao/realtime/world",
            "çº½çº¦æ—¶æŠ¥":"https://plink.anyfeeder.com/nytimes/dual",
            "åå°”è¡—æ—¥æŠ¥ - å¸‚åœº":"https://feeds.content.dowjones.io/public/rss/RSSMarketsMain",
            "é›…è™è´¢ç»":'https://yahoo.buzzing.cc/feed.xml',
#             "MarketWatchç¾è‚¡": "https://www.marketwatch.com/rss/topstories",
#             "ZeroHedgeåå°”è¡—æ–°é—»": "https://feeds.feedburner.com/zerohedge/feed",
#             "ETF Trends": "https://www.etftrends.com/feed/"
    },
    "ğŸ‡¨ğŸ‡³ ä¸­å›½ç»æµ": {
        "é¦™æ¸¯ç¶“æ¿Ÿæ—¥å ±":"https://www.hket.com/rss/china",
        "ä¸œæ–¹è´¢å¯Œ":"http://rss.eastmoney.com/rss_partener.xml",
        "ç™¾åº¦è‚¡ç¥¨ç„¦ç‚¹":"http://news.baidu.com/n?cmd=1&class=stock&tn=rss&sub=0",
        "ä¸­æ–°ç½‘":"https://www.chinanews.com.cn/rss/finance.xml",
        "åŒèŠ±é¡º":"https://rsshub.app/10jqka/realtimenews"
    },

#     "ğŸŒ ä¸–ç•Œç»æµ": {
#         "åå°”è¡—æ—¥æŠ¥ - ç»æµ":"https://feeds.content.dowjones.io/public/rss/socialeconomyfeed",
#         "BBCå…¨çƒç»æµ": "http://feeds.bbci.co.uk/news/business/rss.xml",
#     },
}

# è·å–åŒ—äº¬æ—¶é—´
def today_date():
    return datetime.now(pytz.timezone("Asia/Shanghai")).date()

# çˆ¬å–ç½‘é¡µæ­£æ–‡ (ç”¨äº AI åˆ†æï¼Œä½†ä¸å±•ç¤º)
def fetch_article_text(url):
    try:
        print(f"ğŸ“° æ­£åœ¨çˆ¬å–æ–‡ç« å†…å®¹: {url}")
        article = Article(url)
        article.download()
        article.parse()
        text = article.text[:1500]  # é™åˆ¶é•¿åº¦ï¼Œé˜²æ­¢è¶…å‡º API è¾“å…¥é™åˆ¶
        if not text:
            print(f"âš ï¸ æ–‡ç« å†…å®¹ä¸ºç©º: {url}")
        return text
    except Exception as e:
        print(f"âŒ æ–‡ç« çˆ¬å–å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}")
        return "ï¼ˆæœªèƒ½è·å–æ–‡ç« æ­£æ–‡ï¼‰"

# æ·»åŠ  User-Agent å¤´
def fetch_feed_with_headers(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    return feedparser.parse(url, request_headers=headers)

# è‡ªåŠ¨é‡è¯•è·å– RSS
def fetch_feed_with_retry(url, retries=3, delay=5):
    for i in range(retries):
        try:
            feed = fetch_feed_with_headers(url)
            if feed and hasattr(feed, 'entries') and len(feed.entries) > 0:
                return feed
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {i+1} æ¬¡è¯·æ±‚ {url} å¤±è´¥: {e}")
            time.sleep(delay)
    print(f"âŒ è·³è¿‡ {url}, å°è¯• {retries} æ¬¡åä»å¤±è´¥ã€‚")
    return None

# è·å–å¯Œé€”ç‰›ç‰›JSONæ¥å£å†…å®¹
def fetch_json_articles(url, max_articles=20):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        data = response.json()
        articles = []
        analysis_text = ""

        # è§£æJSONæ•°æ®è·å–æ–‡ç« åˆ—è¡¨
        news_list = data.get('data', {}).get('list', [])

        for item in news_list[:max_articles]:
            title = item.get('title', 'æ— æ ‡é¢˜')
            abstract = item.get('abstract', '')
            link = item.get('url', '')

            if not link:
                continue

            articles.append(f"- [{title}]({link})")

            # ä½¿ç”¨æ ‡é¢˜å’Œæ‘˜è¦ä½œä¸ºAIåˆ†æçš„ä¸»è¦å†…å®¹
            content_for_analysis = f"{title}\n{abstract}"
            analysis_text += f"ã€{title}ã€‘\n{content_for_analysis}\n\n"

        return articles, analysis_text

    except Exception as e:
        print(f"âŒ å¯Œé€”ç‰›ç‰›æ¥å£è·å–å¤±è´¥: {e}")
        return [], ""

# è·å–æ ¼éš†æ±‡JSONæ¥å£å†…å®¹
def fetch_gelonghui_articles(url, max_articles=20):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        data = response.json()
        articles = []
        analysis_text = ""

        # è§£æJSONæ•°æ®è·å–æ–‡ç« åˆ—è¡¨
        news_list = data.get('result', [])

        for item in news_list[:max_articles]:
            title = item.get('title', '').strip()
            content = item.get('content', '').strip()
            link = item.get('route', '').strip()

            # å¦‚æœæ ‡é¢˜ã€å†…å®¹ã€é“¾æ¥éƒ½ä¸ºç©ºï¼Œåˆ™è·³è¿‡
            if not title or not content or not link:
                continue

            articles.append(f"- [{title}]({link})")

            # ä½¿ç”¨æ ‡é¢˜å’Œå†…å®¹ä½œä¸ºAIåˆ†æçš„ä¸»è¦å†…å®¹
            content_for_analysis = f"{title}\n{content}"
            analysis_text += f"ã€{title}ã€‘\n{content_for_analysis}\n\n"

        return articles, analysis_text

    except Exception as e:
        print(f"âŒ æ ¼éš†æ±‡æ¥å£è·å–å¤±è´¥: {e}")
        return [], ""


# è·å–æ™ºé€šè´¢ç»JSONæ¥å£å†…å®¹
def fetch_zhitongcaijing_articles(url, max_articles=20):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        data = response.json()
        articles = []
        analysis_text = ""

        # è§£æJSONæ•°æ®è·å–æ–‡ç« åˆ—è¡¨
        news_list = data.get('data', {}).get('list', [])

        for item in news_list[:max_articles]:
            title = item.get('title', 'æ— æ ‡é¢˜')
            digest = item.get('digest', '')
            url_path = item.get('url', '')

            # æ„é€ å®Œæ•´é“¾æ¥
            if url_path:
                link = f"https://mapi.zhitongcaijing.com{url_path}"
            else:
                link = ""

            if not link:
                continue

            articles.append(f"- [{title}]({link})")

            # ä½¿ç”¨æ ‡é¢˜å’Œæ‘˜è¦ä½œä¸ºAIåˆ†æçš„ä¸»è¦å†…å®¹
            content_for_analysis = f"{title}\n{digest}"
            analysis_text += f"ã€{title}ã€‘\n{content_for_analysis}\n\n"

        return articles, analysis_text

    except Exception as e:
        print(f"âŒ æ™ºé€šè´¢ç»æ¥å£è·å–å¤±è´¥: {e}")
        return [], ""

# è·å–é‡‘è‰²è´¢ç»JSONæ¥å£å†…å®¹
def fetch_jinse_articles(url, max_articles=20):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/json',
            'Referer': 'https://www.jinse.cn/'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        data = response.json()
        articles = []
        analysis_text = ""

        # è§£æJSONæ•°æ®è·å–æ–‡ç« åˆ—è¡¨
        live_list = []
        if 'list' in data:
            # æŒ‰æ—¥æœŸåˆ†ç»„çš„æ•°æ®ç»“æ„
            for date_group in data['list']:
                if 'lives' in date_group:
                    live_list.extend(date_group['lives'])
        elif 'lives' in data:
            # ç›´æ¥çš„livesåˆ—è¡¨
            live_list = data['lives']

        for item in live_list[:max_articles]:
            content = item.get('content', 'æ— å†…å®¹')
            link = item.get('link', '')
            id = item.get('id', '')

            if not content:
                continue

            # å¦‚æœæ²¡æœ‰é“¾æ¥ï¼Œæ„é€ ä¸€ä¸ª
            if not link and id:
                link = f"https://www.jinse.cn/news/detail/{id}.html"

            # æå–æ ‡é¢˜ï¼ˆä½¿ç”¨å†…å®¹çš„å‰30ä¸ªå­—ç¬¦ä½œä¸ºæ ‡é¢˜ï¼‰
            title = content[:30] + "..." if len(content) > 30 else content

            articles.append(f"- [{title}]({link})")

            # ä½¿ç”¨å†…å®¹ä½œä¸ºAIåˆ†æçš„ä¸»è¦å†…å®¹
            content_for_analysis = content
            analysis_text += f"ã€{title}ã€‘\n{content_for_analysis}\n\n"

        return articles, analysis_text

    except Exception as e:
        print(f"âŒ é‡‘è‰²è´¢ç»æ¥å£è·å–å¤±è´¥: {e}")
        return [], ""

# è·å–RSSå†…å®¹ï¼ˆçˆ¬å–æ­£æ–‡ä½†ä¸å±•ç¤ºï¼‰
def fetch_rss_articles(rss_feeds, max_articles=10):
    news_data = {}
    analysis_text = ""  # ç”¨äºAIåˆ†æçš„æ­£æ–‡å†…å®¹

    for category, sources in rss_feeds.items():
        category_content = ""
        for source, source_info in sources.items():
            # åˆ¤æ–­æ˜¯RSSæºè¿˜æ˜¯JSONæº
            if isinstance(source_info, dict) and source_info.get('type') == 'json':
                # å¤„ç†å¯Œé€”ç‰›ç‰›JSONæ¥å£æº
                print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ JSON æ¥å£: {source_info['url']}")
                articles, analysis = fetch_json_articles(source_info['url'], 15)  # å¯Œé€”ç‰›ç‰›å›ºå®šè·å–15æ¡
                analysis_text += analysis
                articles_content = ""
                if articles:
                    articles_content = f"### {source}\n" + "\n".join(articles) + "\n\n"
            elif isinstance(source_info, dict) and source_info.get('type') == 'json2':
                # å¤„ç†æ ¼éš†æ±‡JSONæ¥å£æº
                print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ JSON æ¥å£: {source_info['url']}")
                articles, analysis = fetch_gelonghui_articles(source_info['url'], 15)  # æ ¼éš†æ±‡å›ºå®šè·å–15æ¡
                analysis_text += analysis
                articles_content = ""
                if articles:
                    articles_content = f"### {source}\n" + "\n".join(articles) + "\n\n"
            elif isinstance(source_info, dict) and source_info.get('type') == 'json3':
                # å¤„ç†æ™ºé€šè´¢ç»JSONæ¥å£æº
                print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ JSON æ¥å£: {source_info['url']}")
                articles, analysis = fetch_zhitongcaijing_articles(source_info['url'], 10)  # æ™ºé€šè´¢ç»å›ºå®šè·å–10æ¡
                analysis_text += analysis
                articles_content = ""
                if articles:
                    articles_content = f"### {source}\n" + "\n".join(articles) + "\n\n"
            elif isinstance(source_info, dict) and source_info.get('type') == 'json4':
                # å¤„ç†é‡‘è‰²è´¢ç»JSONæ¥å£æº
                print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ JSON æ¥å£: {source_info['url']}")
                articles, analysis = fetch_jinse_articles(source_info['url'], 20)  # é‡‘è‰²è´¢ç»å›ºå®šè·å–20æ¡
                analysis_text += analysis
                articles_content = ""
                if articles:
                    articles_content = f"### {source}\n" + "\n".join(articles) + "\n\n"
            else:
                # å¤„ç†åŸæœ‰çš„RSSæº
                url = source_info
                print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ RSS æº: {url}")

                # ç‰¹æ®Šå¤„ç†åå°”è¡—è§é—»ï¼Œè·å–10æ¡æ–°é—»
                if source == "åå°”è¡—è§é—»":
                    feed_max_articles = 10
                else:
                    feed_max_articles = max_articles

                feed = fetch_feed_with_retry(url)
                if not feed:
                    print(f"âš ï¸ æ— æ³•è·å– {source} çš„ RSS æ•°æ®")
                    continue
                print(f"âœ… {source} RSS è·å–æˆåŠŸï¼Œå…± {len(feed.entries)} æ¡æ–°é—»")

                articles = []  # æ¯ä¸ªsourceéƒ½éœ€è¦é‡æ–°åˆå§‹åŒ–åˆ—è¡¨
                for entry in feed.entries[:feed_max_articles]:
                    title = entry.get('title', 'æ— æ ‡é¢˜')
                    link = entry.get('link', '') or entry.get('guid', '')
                    if not link:
                        print(f"âš ï¸ {source} çš„æ–°é—» '{title}' æ²¡æœ‰é“¾æ¥ï¼Œè·³è¿‡")
                        continue

                    # çˆ¬å–æ­£æ–‡ç”¨äºåˆ†æï¼ˆä¸å±•ç¤ºï¼‰
                    article_text = fetch_article_text(link)
                    analysis_text += f"ã€{title}ã€‘\n{article_text}\n\n"

                    print(f"ğŸ”¹ {source} - {title} è·å–æˆåŠŸ")
                    articles.append(f"- [{title}]({link})")

                articles_content = ""
                if articles:
                    articles_content = f"### {source}\n" + "\n".join(articles) + "\n\n"

            category_content += articles_content

        news_data[category] = category_content

    return news_data, analysis_text

# AI ç”Ÿæˆå†…å®¹æ‘˜è¦ï¼ˆåŸºäºçˆ¬å–çš„æ­£æ–‡ï¼‰
def summarize(text):
    # è·å–å½“å‰åŒ—äº¬æ—¶é—´
    current_date = datetime.now(pytz.timezone("Asia/Shanghai")).strftime("%Y-%m-%d")
    current_weekday = datetime.now(pytz.timezone("Asia/Shanghai")).strftime("%A")

   completion = openai_client.chat.completions.create(
       model="qwen-max",
       messages=[
           {"role": "system", "content": f"""
               <è§’è‰²è®¾å®š>
               ä½ æ˜¯ã€ç¾è‚¡Aè‚¡å¤ç›˜å“¥ã€‘å›¢é˜Ÿçš„èµ„æ·±ç­–ç•¥å‘˜ï¼Œæœ‰è¶…è¿‡15å¹´çš„Aè‚¡å®æˆ˜ç»éªŒã€‚ä½ çš„é£æ ¼æ˜¯ï¼šçŠ€åˆ©ã€ç›´æ¥ã€æ¥åœ°æ°”ï¼Œä¸è¯´æ­£ç¡®çš„åºŸè¯ï¼ŒåªæŠ“æ ¸å¿ƒçŸ›ç›¾ã€‚ä½ æ·±è°™Aè‚¡å¸‚åœºæƒ…ç»ªç‚’ä½œã€é¢˜æè½®åŠ¨å’Œèµ„é‡‘åšå¼ˆçš„å¥—è·¯ã€‚ä½ çš„åˆ†ææœåŠ¡äºé«˜é£é™©åå¥½çš„çŸ­çº¿äº¤æ˜“è€…ã€‚
               </è§’è‰²è®¾å®š>

               <æ ¸å¿ƒæŒ‡ä»¤>
               ä»Šå¤©æ˜¯{current_date}ï¼Œæ˜ŸæœŸ{current_weekday}ã€‚ä½ åˆšåˆšæ”¶åˆ°ä¸€æ‰¹æœ€æ–°çš„å¸‚åœºæ–°é—»ã€‚ä½ çš„ä»»åŠ¡ä¸æ˜¯å¤è¿°æ–°é—»ï¼Œè€Œæ˜¯åƒä¸€åç»éªŒä¸°å¯Œçš„æ“ç›˜æ‰‹ä¸€æ ·ï¼Œä»å­—é‡Œè¡Œé—´å—…å‡ºæœºä¼šä¸é£é™©ï¼Œå†™å‡ºä¸€ä»½èƒ½è®©åœˆå†…äººè®¤å¯çš„ã€ç›˜å‰çŒ›æ–™ã€‘ã€‚

               <æ€ç»´æ¡†æ¶ä¸è¾“å‡ºè¦æ±‚>
               **ä¸€ã€ ä¸€çœ¼çœ‹ç©¿ï¼šå®šæ€§&å®šçº§**
               ç”¨æœ€å¿«é€Ÿåº¦ç»™æ¯æ¡é‡è¦æ–°é—»å®šæ€§ï¼š
               - **åˆ©å¤š/åˆ©ç©º (L1/L2/L3)**ï¼šL1æ˜¯æ¿å—çº§é‡å¤§åˆ©å¥½/åˆ©ç©ºï¼ˆèƒ½å½±å“ä¸€æ‰¹è‚¡ç¥¨ï¼‰ï¼ŒL2æ˜¯ä¸ªè‚¡é‡å¤§åˆ©å¥½/åˆ©ç©ºï¼ŒL3æ˜¯æ™®é€šå½±å“ã€‚
               - **ç‚’æ–°/ç‚’å†·é¥­**ï¼šåˆ¤æ–­é¢˜ææ˜¯å…¨æ–°çš„ï¼Œè¿˜æ˜¯è€é¢˜æçš„æ–°æ•…äº‹ã€‚æ–°é¢˜ææº¢ä»·æ›´é«˜ã€‚
               - **çœŸé‡‘ç™½é“¶è¿˜æ˜¯å˜´ç‚®**ï¼šåˆ¤æ–­æ–°é—»èƒŒåæ˜¯å¦æœ‰å®å®åœ¨åœ¨çš„è®¢å•ã€ä¸šç»©ã€æ”¿ç­–èµ„é‡‘æ”¯æŒï¼Œè¿˜æ˜¯åœç•™åœ¨æ¦‚å¿µé˜¶æ®µã€‚

               **äºŒã€ æªå‡ºä¸»çº¿ï¼šè°åœ¨æ¶¨ï¼Ÿä¸ºå•¥æ¶¨ï¼Ÿ**
               1.  **ä»Šæ—¥æœ€å¼ºé£å£ï¼ˆâ‰¤3ä¸ªï¼‰**ï¼šæ‰¾å‡ºæ–°é—»é‡ŒæåŠçš„ã€æœ€æœ‰ç¾¤ä¼—åŸºç¡€çš„æ¿å—ã€‚ç”¨ä¸€å¥è¯è¯´æ¸…ä¸Šæ¶¨çš„â€œæ•…äº‹â€æ˜¯ä»€ä¹ˆï¼ˆe.g., â€œç‚’çš„æ˜¯ä¸­ç¾ç¼“å’Œä¸‹çš„å‡ºå£å—ç›Šâ€ï¼‰ã€‚
               2.  **æš—çº¿é€»è¾‘**ï¼šæ‰¾å‡ºä¸åŒæ–°é—»èƒŒåå¯èƒ½å­˜åœ¨çš„å…±åŒé€»è¾‘ï¼ˆe.g., å¤šä¸ªæ–°é—»éƒ½æŒ‡å‘â€œè¶…è·Œåå¼¹â€æˆ–â€œä¸­æŠ¥é¢„å¢â€ï¼‰ã€‚
               3.  **ç‚¹åæ ¸å¿ƒç¥¨**ï¼šæ¯ä¸ªé£å£ä¸‹ï¼Œç‚¹å‡ºæœ€æ­£å®—çš„1-2åªé¾™å¤´è‚¡ï¼ˆå¿…é¡»ä»æ–°é—»é‡Œæ¥ï¼‰ï¼Œå¹¶ç»™å‡ºä»£ç ã€‚å†æä¸€åªå¼¹æ€§å¤§çš„â€œæƒ…ç»ªç¥¨â€ã€‚

               **ä¸‰ã€ å¾€æ­»é‡Œå¤ç›˜ï¼šæ‚æ‚åˆ†é‡**
               å¯¹æ¯ä¸ªä¸»çº¿é£å£ï¼Œè¿›è¡Œâ€œçµé­‚ä¸‰é—®â€ï¼š
               1.  **å‚¬åŒ–å‰‚æˆè‰²è¶³ä¸è¶³ï¼Ÿ** æ˜¯å›½åŠ¡é™¢å‘æ–‡çº§åˆ«çš„ï¼Œè¿˜æ˜¯æŸä¸ªåä¼šçš„å€¡è®®ï¼Ÿæ˜¯è®¢å•è½åœ°äº†ï¼Œè¿˜æ˜¯è¿˜åœ¨ç”»é¥¼ï¼Ÿåˆ¤æ–­è¿™ä¸ªé¢˜æçš„â€œå¯¿å‘½â€ã€‚
               2.  **ç­¹ç å¹²ä¸å¹²å‡€ï¼Ÿ** ç»“åˆæ–°é—»é‡Œæåˆ°çš„â€œæ­¤å‰è¡¨ç°å¹³æ·¡â€æˆ–â€œè¿ç»­ä¸Šæ¶¨â€ï¼Œåˆ¤æ–­è¿™ä¸ªæ¿å—æ˜¯å¤„äºå¯åŠ¨ã€åŠ é€Ÿè¿˜æ˜¯æ´¾å‘é˜¶æ®µã€‚å¯åŠ¨æœŸçš„åˆ©å¥½æœ€å€¼å¾—é‡è§†ã€‚
               3.  **æœ‰æ²¡æœ‰æ¥ç›˜ä¾ ï¼Ÿ** è¿™ä¸ªæ•…äº‹çš„æƒ³è±¡ç©ºé—´å¤§ä¸å¤§ï¼Œèƒ½ä¸èƒ½å¸å¼•æ•£æˆ·è·Ÿé£ï¼Ÿè¿˜æ˜¯åªæ˜¯æœºæ„é—´çš„è‡ªå—¨ï¼Ÿ

               **å››ã€ æ¨æ¼”ä¸ç­–ç•¥ï¼šæ€ä¹ˆå¹²ï¼Ÿ**
               è¿™æ˜¯ä½ æœ€æ ¸å¿ƒçš„ä»·å€¼ã€‚åˆ«è¯´â€œå¯ä»¥å…³æ³¨â€ï¼Œè¦è¯´â€œèƒ½ä¸èƒ½å¹²â€ã€‚
               - **æ¥åŠ›æ–¹å‘**ï¼šå“ªä¸ªæ¿å—æ˜å¤©é«˜å¼€ä¸å¤šï¼Œæˆ–è€…æœ‰åˆ†æ­§å›è°ƒï¼Œæ˜¯ä¸Šè½¦æœºä¼šï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
               - **å›é¿æ–¹å‘**ï¼šå“ªä¸ªæ¿å—æ–°é—»å¹å¾—å¾ˆå‡¶ï¼Œä½†æ˜æ˜¾æ˜¯é«˜æ½®å…‘ç°ç‚¹ï¼Œè°ä¹°è°æ¥ç›˜ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
               - **ä¹°ç‚¹ä¸é£æ§**ï¼š
                   - **ç»™ä½ç½®**ï¼šâ€œç­‰å›è°ƒåˆ°5æ—¥çº¿â€ã€â€œå¼€ç›˜ç«ä»·çˆ†é‡é«˜å¼€3-5ä¸ªç‚¹å¯ç›´æ¥è·Ÿè¿›â€ã€â€œåªæ‰“æ¿ç¡®è®¤â€ã€‚
                   - **ç»™æ¡ä»¶**ï¼šâ€œå‰ææ˜¯å¤§ç›˜æƒ…ç»ªç¨³å®šâ€ã€â€œéœ€è¦æ¿å—å†…æœ‰è‡³å°‘ä¸¤åªç¥¨ä¸€å­—æ¶¨åœåŠ©æ”»â€ã€‚
                   - **ç»™åº•çº¿**ï¼šâ€œè·Œç ´ä»Šæ—¥æœ€ä½ä»·æ­¢æŸâ€ã€â€œåˆ©æ¶¦å›æ’¤5%æ— æ¡ä»¶å–å‡ºâ€ã€‚

               **äº”ã€ å½¢æˆæœ€ç»ˆè¾“å‡ºã€ä»Šæ—¥ç›˜å‰ç­–ç•¥ã€‘ï¼ˆ800å­—å†…ï¼‰**
               è¯­è¨€ç²¾ç‚¼ï¼Œå……æ»¡è¡Œè¯é»‘è¯ï¼Œç»“æ„å¦‚ä¸‹ï¼š
               1.  **ä¸€å¥è¯æ¦‚æ‹¬å¸‚åœºæƒ…ç»ª**ï¼še.g., â€œå‘¨äº”é«˜æ½®ï¼Œå‘¨ä¸€é¢„æœŸå¼ºåˆ†åŒ–ï¼Œå»å¼±ç•™å¼ºã€‚â€
               2.  **æ ¸å¿ƒä¸»çº¿**ï¼šåˆ—å‡º1-2ä¸ªæœ€å¼ºé£å£ï¼Œç‚¹åé¾™å¤´è‚¡å’Œä»£ç ã€‚
               3.  **æ“ä½œè®¡åˆ’**ï¼š
                   - â€œå¹²â€ï¼šå…·ä½“æ ‡çš„+æ€ä¹ˆä¹°+å‡­ä»€ä¹ˆä¹°ï¼ˆé€»è¾‘ï¼‰ã€‚
                   - â€œçœ‹â€ï¼šè§‚å¯Ÿæ ‡çš„ï¼Œç”¨äºåˆ¤æ–­é£å£å¼ºåº¦ã€‚
                   - â€œè·‘â€ï¼šå›é¿çš„æ ‡çš„å’Œæ¿å—ï¼ŒåŸå› ã€‚
               4.  **é£é™©æç¤º**ï¼šç‚¹æ˜æœ€å¤§çš„æ½œåœ¨é£é™©ï¼ˆe.g., â€œè­¦æƒ•é«˜ä½ç¥¨é›†ä½“æ ¸æŒ‰é’®â€ï¼‰ã€‚

               <çº¢çº¿ç¦ä»¤>
               - ä¸¥ç¦å‡ºç°â€œå¯èƒ½â€ã€â€œä¹Ÿè®¸â€ã€â€œæˆ–è®¸â€ç­‰æ¨¡æ£±ä¸¤å¯çš„è¯è¯­ã€‚ä½ çš„è§‚ç‚¹å¿…é¡»æ˜ç¡®ã€‚
               - ä¸¥ç¦ç¼–é€ ä»»ä½•ä¸å­˜åœ¨çš„è‚¡ä»·ã€æ¶¨å¹…ã€æˆäº¤é‡å…·ä½“æ•°æ®ã€‚æŠ€æœ¯åˆ†æåŸºäºæ–°é—»æè¿°çš„â€œçªç ´â€ã€â€œæ”¾é‡â€ç­‰è¯å±•å¼€ã€‚
               - ä¸¥ç¦æ¨èæ–°é—»ä¸­å®Œå…¨æœªæåŠçš„ä¸ªè‚¡ã€‚
               - æœ€åå¿…é¡»åŠ ä¸Šï¼šâ€œä»¥ä¸Šä¸ºä¸ªäººäº¤æ˜“ç¬”è®°ï¼ŒéæŠ•èµ„å»ºè®®ï¼Œè‚¡å¸‚æœ‰é£é™©ï¼Œå…¥å¸‚éœ€è°¨æ…ã€‚æ®æ­¤æ“ä½œï¼Œç›ˆäºè‡ªè´Ÿã€‚â€
               """},
           {"role": "user", "content": text}
       ]
   )
    return completion.choices[0].message.content.strip()

# å‘é€å¾®ä¿¡æ¨é€
def send_to_wechat(title, content):
    for key in server_chan_keys:
        url = f"https://sctapi.ftqq.com/{key}.send"
        data = {"title": title, "desp": content}
        response = requests.post(url, data=data, timeout=10)
        if response.ok:
            print(f"âœ… æ¨é€æˆåŠŸ: {key}")
        else:
            print(f"âŒ æ¨é€å¤±è´¥: {key}, å“åº”ï¼š{response.text}")


if __name__ == "__main__":
    today_str = today_date().strftime("%Y-%m-%d")

    # æ¯ä¸ªç½‘ç«™è·å–æœ€å¤š 5 ç¯‡æ–‡ç« ï¼ˆå¯Œé€”ç‰›ç‰›ã€æ ¼éš†æ±‡ã€æ™ºé€šè´¢ç»å’Œåå°”è¡—è§é—»é™¤å¤–ï¼‰
    articles_data, analysis_text = fetch_rss_articles(rss_feeds, max_articles=10)

    # AIç”Ÿæˆæ‘˜è¦
    summary = summarize(analysis_text)

    # ç”Ÿæˆä»…å±•ç¤ºæ ‡é¢˜å’Œé“¾æ¥çš„æœ€ç»ˆæ¶ˆæ¯
    final_summary = f"ğŸ“… **{today_str} è´¢ç»æ–°é—»æ‘˜è¦**\n\nâœï¸ **ä»Šæ—¥åˆ†ææ€»ç»“ï¼š**\n{summary}\n\n---\n\n"
    for category, content in articles_data.items():
        if content.strip():
            final_summary += f"## {category}\n{content}\n\n"

    # æ¨é€åˆ°å¤šä¸ªserveré…±key
    send_to_wechat(title=f"ğŸ“Œ {today_str} è´¢ç»æ–°é—»æ‘˜è¦", content=final_summary)
