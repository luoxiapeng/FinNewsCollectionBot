# ç¦ç”Ÿæ— é‡å¤©å°Š
from openai import OpenAI
import feedparser
import requests
from newspaper import Article
from datetime import datetime
import time
import pytz
import os
# OpenAI API Key
# openai_api_key = os.getenv("OPENAI_API_KEY")
# ä»ç¯å¢ƒå˜é‡è·å– Serveré…± SendKeys
server_chan_keys_env = os.getenv("SERVER_CHAN_KEYS")
if not server_chan_keys_env:
    raise ValueError("ç¯å¢ƒå˜é‡ SERVER_CHAN_KEYS æœªè®¾ç½®ï¼Œè¯·åœ¨Github Actionsä¸­è®¾ç½®æ­¤å˜é‡ï¼")
server_chan_keys = server_chan_keys_env.split(",")

# openai_client = OpenAI(api_key=openai_api_key, base_url="https://api.deepseek.com/v1")
# è·å–DashScope API Key
dashscope_api_key = os.getenv("DASHSCOPE_API_KEY")
if not dashscope_api_key:
    raise ValueError("ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY æœªè®¾ç½®ï¼Œè¯·åœ¨Github Actionsä¸­è®¾ç½®æ­¤å˜é‡ï¼")

openai_client = OpenAI(
    api_key=dashscope_api_key,
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

# RSSæºåœ°å€åˆ—è¡¨
rss_feeds = {
    "ğŸ“ˆ å¯Œé€”ç‰›ç‰›": {
        "å¯Œé€”ç‰›ç‰›å¸‚åœºèµ„è®¯": {
            "url": "https://news.futunn.com/news-site-api/main/get-market-list?size=15",
            "type": "json"
        }
    },
    "ğŸ“Š æ ¼éš†æ±‡": {
        "æ ¼éš†æ±‡è´¢ç»": {
            "url": "https://www.gelonghui.com/api/live-channels/all/lives/v4?category=all&limit=15",
            "type": "json2"
        }
    },
    "ğŸ’°æ™ºé€šè´¢ç»": {
        "æ™ºé€šè´¢ç»": {
            "url": "https://mapi.zhitongcaijing.com/news/list.html?mode=history&access_token=&category_id=index_shouye&category_key=&language=zh-cn&last_time=&page=1&tradition_chinese=0",
            "type": "json3"
        }
    },
    "ğŸŒ é‡‘è‰²è´¢ç»": {
            "é‡‘è‰²è´¢ç»å¿«è®¯": {
                "url": "https://api.jinse.cn/noah/v2/lives?limit=20&reading=false&source=web&flag=up&id=0&category=0",
                "type": "json4"
            }
      },
    "ğŸ’²åå°”è¡—è§é—»":{
        "åå°”è¡—è§é—»":"https://dedicated.wallstreetcn.com/rss.xml",
    },
     "å½­åšç¤¾":{
          "å½­åšç¤¾":"https://bloombergnew.buzzing.cc/feed.xml"
     },
    "ğŸ’» 36æ°ª":{
        "36æ°ª":"https://36kr.com/feed",
    },
    "è´¢è”ç¤¾":{
          "è´¢è”ç¤¾å¤´æ¡":' https://rsshub.app/cls/telegraph/red',
          "è´¢è”ç¤¾çƒ­é—¨":' https://rsshub.app/cls/hot'
    },
     "ğŸ‡ºğŸ‡¸ ç¾å›½ç»æµ": {
            "åå°”è¡—æ—¥æŠ¥ - ç»æµ":"https://cn.wsj.com/zh-hans/rss",
            "è”åˆæ—©æŠ¥":"https://plink.anyfeeder.com/zaobao/realtime/world",
            "çº½çº¦æ—¶æŠ¥":"https://plink.anyfeeder.com/nytimes/dual",
#             "åå°”è¡—æ—¥æŠ¥ - å¸‚åœº":"https://feeds.content.dowjones.io/public/rss/RSSMarketsMain",
            "é›…è™è´¢ç»":'https://yahoo.buzzing.cc/feed.xml',
#             "MarketWatchç¾è‚¡": "https://www.marketwatch.com/rss/topstories",
#             "ZeroHedgeåå°”è¡—æ–°é—»": "https://feeds.feedburner.com/zerohedge/feed",
#             "ETF Trends": "https://www.etftrends.com/feed/"
    },
    "ğŸ‡¨ğŸ‡³ ä¸­å›½ç»æµ": {
        "é¦™æ¸¯ç¶“æ¿Ÿæ—¥å ±":"https://www.hket.com/rss/china",
#         "ä¸œæ–¹è´¢å¯Œ":"http://rss.eastmoney.com/rss_partener.xml",
        "ç™¾åº¦è‚¡ç¥¨ç„¦ç‚¹":"http://news.baidu.com/n?cmd=1&class=stock&tn=rss&sub=0",
#         "ä¸­æ–°ç½‘":"https://www.chinanews.com.cn/rss/finance.xml",
        "åŒèŠ±é¡º":"https://rsshub.app/10jqka/realtimenews"
    },

#     "ğŸŒ ä¸–ç•Œç»æµ": {
#         "åå°”è¡—æ—¥æŠ¥ - ç»æµ":"https://feeds.content.dowjones.io/public/rss/socialeconomyfeed",
#         "BBCå…¨çƒç»æµ": "http://feeds.bbci.co.uk/news/business/rss.xml",
#     },
}

# è·å–åŒ—äº¬æ—¶é—´
def today_date():
    return datetime.now(pytz.timezone("Asia/Shanghai")).date()

# çˆ¬å–ç½‘é¡µæ­£æ–‡ (ç”¨äº AI åˆ†æï¼Œä½†ä¸å±•ç¤º)
def fetch_article_text(url):
    try:
        print(f"ğŸ“° æ­£åœ¨çˆ¬å–æ–‡ç« å†…å®¹: {url}")
        article = Article(url)
        article.download()
        article.parse()
        text = article.text[:1500]  # é™åˆ¶é•¿åº¦ï¼Œé˜²æ­¢è¶…å‡º API è¾“å…¥é™åˆ¶
        if not text:
            print(f"âš ï¸ æ–‡ç« å†…å®¹ä¸ºç©º: {url}")
        return text
    except Exception as e:
        print(f"âŒ æ–‡ç« çˆ¬å–å¤±è´¥: {url}ï¼Œé”™è¯¯: {e}")
        return "ï¼ˆæœªèƒ½è·å–æ–‡ç« æ­£æ–‡ï¼‰"

# æ·»åŠ  User-Agent å¤´
def fetch_feed_with_headers(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    return feedparser.parse(url, request_headers=headers)

# è‡ªåŠ¨é‡è¯•è·å– RSS
def fetch_feed_with_retry(url, retries=3, delay=5):
    for i in range(retries):
        try:
            feed = fetch_feed_with_headers(url)
            if feed and hasattr(feed, 'entries') and len(feed.entries) > 0:
                return feed
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {i+1} æ¬¡è¯·æ±‚ {url} å¤±è´¥: {e}")
            time.sleep(delay)
    print(f"âŒ è·³è¿‡ {url}, å°è¯• {retries} æ¬¡åä»å¤±è´¥ã€‚")
    return None

# è·å–å¯Œé€”ç‰›ç‰›JSONæ¥å£å†…å®¹
def fetch_json_articles(url, max_articles=20):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        data = response.json()
        articles = []
        analysis_text = ""

        # è§£æJSONæ•°æ®è·å–æ–‡ç« åˆ—è¡¨
        news_list = data.get('data', {}).get('list', [])

        for item in news_list[:max_articles]:
            title = item.get('title', 'æ— æ ‡é¢˜')
            abstract = item.get('abstract', '')
            link = item.get('url', '')

            if not link:
                continue

            articles.append(f"- [{title}]({link})")

            # ä½¿ç”¨æ ‡é¢˜å’Œæ‘˜è¦ä½œä¸ºAIåˆ†æçš„ä¸»è¦å†…å®¹
            content_for_analysis = f"{title}\n{abstract}"
            analysis_text += f"ã€{title}ã€‘\n{content_for_analysis}\n\n"

        return articles, analysis_text

    except Exception as e:
        print(f"âŒ å¯Œé€”ç‰›ç‰›æ¥å£è·å–å¤±è´¥: {e}")
        return [], ""

# è·å–æ ¼éš†æ±‡JSONæ¥å£å†…å®¹
def fetch_gelonghui_articles(url, max_articles=20):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        data = response.json()
        articles = []
        analysis_text = ""

        # è§£æJSONæ•°æ®è·å–æ–‡ç« åˆ—è¡¨
        news_list = data.get('result', [])

        for item in news_list[:max_articles]:
            title = item.get('title', '').strip()
            content = item.get('content', '').strip()
            link = item.get('route', '').strip()

            # å¦‚æœæ ‡é¢˜ã€å†…å®¹ã€é“¾æ¥éƒ½ä¸ºç©ºï¼Œåˆ™è·³è¿‡
            if not title or not content or not link:
                continue

            articles.append(f"- [{title}]({link})")

            # ä½¿ç”¨æ ‡é¢˜å’Œå†…å®¹ä½œä¸ºAIåˆ†æçš„ä¸»è¦å†…å®¹
            content_for_analysis = f"{title}\n{content}"
            analysis_text += f"ã€{title}ã€‘\n{content_for_analysis}\n\n"

        return articles, analysis_text

    except Exception as e:
        print(f"âŒ æ ¼éš†æ±‡æ¥å£è·å–å¤±è´¥: {e}")
        return [], ""


# è·å–æ™ºé€šè´¢ç»JSONæ¥å£å†…å®¹
def fetch_zhitongcaijing_articles(url, max_articles=20):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        data = response.json()
        articles = []
        analysis_text = ""

        # è§£æJSONæ•°æ®è·å–æ–‡ç« åˆ—è¡¨
        news_list = data.get('data', {}).get('list', [])

        for item in news_list[:max_articles]:
            title = item.get('title', 'æ— æ ‡é¢˜')
            digest = item.get('digest', '')
            url_path = item.get('url', '')

            # æ„é€ å®Œæ•´é“¾æ¥
            if url_path:
                link = f"https://mapi.zhitongcaijing.com{url_path}"
            else:
                link = ""

            if not link:
                continue

            articles.append(f"- [{title}]({link})")

            # ä½¿ç”¨æ ‡é¢˜å’Œæ‘˜è¦ä½œä¸ºAIåˆ†æçš„ä¸»è¦å†…å®¹
            content_for_analysis = f"{title}\n{digest}"
            analysis_text += f"ã€{title}ã€‘\n{content_for_analysis}\n\n"

        return articles, analysis_text

    except Exception as e:
        print(f"âŒ æ™ºé€šè´¢ç»æ¥å£è·å–å¤±è´¥: {e}")
        return [], ""

# è·å–é‡‘è‰²è´¢ç»JSONæ¥å£å†…å®¹
def fetch_jinse_articles(url, max_articles=20):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/json',
            'Referer': 'https://www.jinse.cn/'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        data = response.json()
        articles = []
        analysis_text = ""

        # è§£æJSONæ•°æ®è·å–æ–‡ç« åˆ—è¡¨
        live_list = []
        if 'list' in data:
            # æŒ‰æ—¥æœŸåˆ†ç»„çš„æ•°æ®ç»“æ„
            for date_group in data['list']:
                if 'lives' in date_group:
                    live_list.extend(date_group['lives'])
        elif 'lives' in data:
            # ç›´æ¥çš„livesåˆ—è¡¨
            live_list = data['lives']

        for item in live_list[:max_articles]:
            content = item.get('content', 'æ— å†…å®¹')
            link = item.get('link', '')
            id = item.get('id', '')

            if not content:
                continue

            # å¦‚æœæ²¡æœ‰é“¾æ¥ï¼Œæ„é€ ä¸€ä¸ª
            if not link and id:
                link = f"https://www.jinse.cn/news/detail/{id}.html"

            # æå–æ ‡é¢˜ï¼ˆä½¿ç”¨å†…å®¹çš„å‰30ä¸ªå­—ç¬¦ä½œä¸ºæ ‡é¢˜ï¼‰
            title = content[:30] + "..." if len(content) > 30 else content

            articles.append(f"- [{title}]({link})")

            # ä½¿ç”¨å†…å®¹ä½œä¸ºAIåˆ†æçš„ä¸»è¦å†…å®¹
            content_for_analysis = content
            analysis_text += f"ã€{title}ã€‘\n{content_for_analysis}\n\n"

        return articles, analysis_text

    except Exception as e:
        print(f"âŒ é‡‘è‰²è´¢ç»æ¥å£è·å–å¤±è´¥: {e}")
        return [], ""

# è·å–RSSå†…å®¹ï¼ˆçˆ¬å–æ­£æ–‡ä½†ä¸å±•ç¤ºï¼‰
def fetch_rss_articles(rss_feeds, max_articles=10):
    news_data = {}
    analysis_text = ""  # ç”¨äºAIåˆ†æçš„æ­£æ–‡å†…å®¹

    for category, sources in rss_feeds.items():
        category_content = ""
        for source, source_info in sources.items():
            # åˆ¤æ–­æ˜¯RSSæºè¿˜æ˜¯JSONæº
            if isinstance(source_info, dict) and source_info.get('type') == 'json':
                # å¤„ç†å¯Œé€”ç‰›ç‰›JSONæ¥å£æº
                print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ JSON æ¥å£: {source_info['url']}")
                articles, analysis = fetch_json_articles(source_info['url'], 15)  # å¯Œé€”ç‰›ç‰›å›ºå®šè·å–15æ¡
                analysis_text += analysis
                articles_content = ""
                if articles:
                    articles_content = f"### {source}\n" + "\n".join(articles) + "\n\n"
            elif isinstance(source_info, dict) and source_info.get('type') == 'json2':
                # å¤„ç†æ ¼éš†æ±‡JSONæ¥å£æº
                print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ JSON æ¥å£: {source_info['url']}")
                articles, analysis = fetch_gelonghui_articles(source_info['url'], 15)  # æ ¼éš†æ±‡å›ºå®šè·å–15æ¡
                analysis_text += analysis
                articles_content = ""
                if articles:
                    articles_content = f"### {source}\n" + "\n".join(articles) + "\n\n"
            elif isinstance(source_info, dict) and source_info.get('type') == 'json3':
                # å¤„ç†æ™ºé€šè´¢ç»JSONæ¥å£æº
                print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ JSON æ¥å£: {source_info['url']}")
                articles, analysis = fetch_zhitongcaijing_articles(source_info['url'], 10)  # æ™ºé€šè´¢ç»å›ºå®šè·å–10æ¡
                analysis_text += analysis
                articles_content = ""
                if articles:
                    articles_content = f"### {source}\n" + "\n".join(articles) + "\n\n"
            elif isinstance(source_info, dict) and source_info.get('type') == 'json4':
                # å¤„ç†é‡‘è‰²è´¢ç»JSONæ¥å£æº
                print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ JSON æ¥å£: {source_info['url']}")
                articles, analysis = fetch_jinse_articles(source_info['url'], 20)  # é‡‘è‰²è´¢ç»å›ºå®šè·å–20æ¡
                analysis_text += analysis
                articles_content = ""
                if articles:
                    articles_content = f"### {source}\n" + "\n".join(articles) + "\n\n"
            else:
                # å¤„ç†åŸæœ‰çš„RSSæº
                url = source_info
                print(f"ğŸ“¡ æ­£åœ¨è·å– {source} çš„ RSS æº: {url}")

                # ç‰¹æ®Šå¤„ç†åå°”è¡—è§é—»ï¼Œè·å–10æ¡æ–°é—»
                if source == "åå°”è¡—è§é—»":
                    feed_max_articles = 10
                else:
                    feed_max_articles = max_articles

                feed = fetch_feed_with_retry(url)
                if not feed:
                    print(f"âš ï¸ æ— æ³•è·å– {source} çš„ RSS æ•°æ®")
                    continue
                print(f"âœ… {source} RSS è·å–æˆåŠŸï¼Œå…± {len(feed.entries)} æ¡æ–°é—»")

                articles = []  # æ¯ä¸ªsourceéƒ½éœ€è¦é‡æ–°åˆå§‹åŒ–åˆ—è¡¨
                for entry in feed.entries[:feed_max_articles]:
                    title = entry.get('title', 'æ— æ ‡é¢˜')
                    link = entry.get('link', '') or entry.get('guid', '')
                    if not link:
                        print(f"âš ï¸ {source} çš„æ–°é—» '{title}' æ²¡æœ‰é“¾æ¥ï¼Œè·³è¿‡")
                        continue

                    # çˆ¬å–æ­£æ–‡ç”¨äºåˆ†æï¼ˆä¸å±•ç¤ºï¼‰
                    article_text = fetch_article_text(link)
                    analysis_text += f"ã€{title}ã€‘\n{article_text}\n\n"

                    print(f"ğŸ”¹ {source} - {title} è·å–æˆåŠŸ")
                    articles.append(f"- [{title}]({link})")

                articles_content = ""
                if articles:
                    articles_content = f"### {source}\n" + "\n".join(articles) + "\n\n"

            category_content += articles_content

        news_data[category] = category_content

    return news_data, analysis_text

# AI ç”Ÿæˆå†…å®¹æ‘˜è¦ï¼ˆåŸºäºçˆ¬å–çš„æ­£æ–‡ï¼‰
def summarize(text):
    # è·å–å½“å‰åŒ—äº¬æ—¶é—´
    current_date = datetime.now(pytz.timezone("Asia/Shanghai")).strftime("%Y-%m-%d")
    current_weekday = datetime.now(pytz.timezone("Asia/Shanghai")).strftime("%A")

    completion = openai_client.chat.completions.create(
        model="qwen-max",  # æˆ–å…¶ä»–ä½ é€‰æ‹©çš„æ¨¡å‹
        messages=[
           {"role": "system", "content": f"""
          # è§’è‰²å®šä½
          æ‚¨æ˜¯ä¸€åæ‹¥æœ‰10å¹´ç»éªŒçš„é¦–å¸­ç­–ç•¥å¸ˆï¼Œä¸“æ³¨äºä¸­å›½è‚¡ç¥¨å¸‚åœºçš„è¡Œä¸šè½®åŠ¨ä¸ä¸»é¢˜æŠ•èµ„åˆ†æã€‚æ‚¨æ“…é•¿ä»æ–°é—»æ–‡æœ¬ä¸­æå–å…³é”®ä¿¡å·ï¼Œå¹¶ç»“åˆå¸‚åœºç¯å¢ƒåšå‡ºä¸“ä¸šåˆ¤æ–­ã€‚

          # åˆ†æèƒŒæ™¯
          å½“å‰æ—¥æœŸï¼š{current_date}ï¼Œæ˜ŸæœŸ{current_weekday}
          æ•°æ®æ¥æºï¼šå½“æ—¥å¸‚åœºæ–°é—»ä¸æ–‡æœ¬æ•°æ®

          # æ ¸å¿ƒä»»åŠ¡
          è¯·åŸºäºä»¥ä¸‹ç»“æ„åŒ–æ¡†æ¶å¯¹æä¾›çš„æ–°é—»å†…å®¹è¿›è¡Œä¸“ä¸šåˆ†æï¼š

          ## 1. çƒ­ç‚¹è¯†åˆ«ä¸ä¸»é¢˜æå–
          - è¯†åˆ«æ–°é—»ä¸­æ¶‰åŠçš„ä¸»è¦è¡Œä¸š/ä¸»é¢˜ï¼ˆæŒ‰ç”³ä¸‡ä¸€çº§è¡Œä¸šåˆ†ç±»ï¼‰
          - ç­›é€‰å‡ºè¿‘æœŸï¼ˆ1-3å¤©ï¼‰å…·æœ‰åŠ¨é‡æ•ˆåº”çš„3ä¸ªæ ¸å¿ƒçƒ­ç‚¹
          - ç‰¹åˆ«å…³æ³¨æ­¤å‰2å‘¨è¡¨ç°å¹³æ·¡ä½†è¿‘æœŸå¼€å§‹å¼‚åŠ¨çš„æ½œåœ¨æ–°çƒ­ç‚¹

          ## 2. æ–°é—»äº‹ä»¶å½±å“åˆ†æ
          å¯¹æ¯æ¡é‡è¦æ–°é—»è¿›è¡Œä¸‰ç»´åº¦è¯„ä¼°ï¼š
          - å½±å“æ€§è´¨ï¼šåˆ©å¥½/åˆ©ç©º/ä¸­æ€§ï¼ˆç”¨â–²/â–¼/â—è¡¨ç¤ºï¼‰
          - å½±å“ç¨‹åº¦ï¼šé«˜/ä¸­/ä½ï¼ˆç”¨â­çº§è¡¨ç¤ºï¼‰
          - å½±å“èŒƒå›´ï¼šä¸ªè‚¡å±‚é¢/è¡Œä¸šå±‚é¢/å¸‚åœºå±‚é¢
          - æœ€å—å½±å“çš„2-3ä¸ªæ ‡çš„åŠç®€è¦é€»è¾‘

          ## 3. æ·±åº¦åˆ†ææ¡†æ¶ï¼ˆå¯¹æ¯ä¸ªé‡ç‚¹è¡Œä¸š/ä¸»é¢˜ï¼‰
          ### å‚¬åŒ–å‰‚åˆ†æ
          - æ”¿ç­–å‚¬åŒ–ï¼šäº§ä¸šæ”¿ç­–ã€åŒºåŸŸè§„åˆ’ã€è¡Œä¸šè§„èŒƒ
          - æ•°æ®å‚¬åŒ–ï¼šä¸šç»©è¶…é¢„æœŸã€è¡Œä¸šæ•°æ®äº®çœ¼
          - äº‹ä»¶å‚¬åŒ–ï¼šæŠ€æœ¯çªç ´ã€è®¢å•è½åœ°ã€æˆ˜ç•¥åˆä½œ
          - æƒ…ç»ªå‚¬åŒ–ï¼šå¸‚åœºé£é™©åå¥½å˜åŒ–ã€èµ„é‡‘æµå‘

          ### è¡Œæƒ…å¤ç›˜ä¸é€»è¾‘æ¢³ç†
          - è¿‡å»3ä¸ªæœˆè¡Œä¸š/ä¸»é¢˜çš„æ ¸å¿ƒé©±åŠ¨é€»è¾‘æ¼”å˜
          - å…³é”®æ—¶é—´èŠ‚ç‚¹çš„å¸‚åœºè¡¨ç°ï¼ˆä¸è¦ä½¿ç”¨å…·ä½“ä»·æ ¼ï¼Œç”¨"æ˜¾è‘—è·‘èµ¢/è·‘è¾“å¤§ç›˜"ç­‰ç›¸å¯¹è¡¨è¿°ï¼‰
          - å½“å‰æ‰€å¤„é˜¶æ®µï¼šå¯åŠ¨æœŸ/åŠ é€ŸæœŸ/åˆ†åŒ–æœŸ/é€€æ½®æœŸ

          ### æŒç»­æ€§å±•æœ›
          åŸºäºä»¥ä¸‹ç»´åº¦åˆ¤æ–­çƒ­ç‚¹å¯æŒç»­æ€§ï¼š
          - æ”¿ç­–æ”¯æŒåŠ›åº¦ä¸æŒç»­æ€§
          - è¡Œä¸šåŸºæœ¬é¢æ”¹å–„ç¨‹åº¦
          - èµ„é‡‘ä»‹å…¥æ·±åº¦ï¼ˆä»æ–°é—»ä¸­æ¨æ–­ï¼‰
          - æŠ€æœ¯é¢ä½ç½®ï¼ˆç›¸å¯¹é«˜ä½ï¼‰

          ### æ ‡çš„æ¨è
          - é¾™å¤´æ ‡çš„ï¼š2-3åªæœ€å…·ä»£è¡¨æ€§çš„ä¸ªè‚¡
          - ETFé€‰é¡¹ï¼šç›¸å…³çš„è¡Œä¸šETFï¼ˆå¦‚æœ‰ï¼‰
          - é¡»åŒ…å«æ ‡çš„ä»£ç å’Œåç§°ï¼ŒæŒ‰é€»è¾‘å…³è”åº¦æ’åº

          ### æŠ€æœ¯åˆ†æè¦ç‚¹
          åŸºäºæ–°é—»ä¸­æåˆ°çš„æŠ€æœ¯ä¿¡å·ï¼š
          - è¶‹åŠ¿çŠ¶æ€ï¼šä¸Šå‡è¶‹åŠ¿/ä¸‹é™è¶‹åŠ¿/éœ‡è¡æ•´ç†
          - å…³é”®ä½ç½®ï¼šæ¥è¿‘é˜»åŠ›ä½/æ”¯æ’‘ä½/çªç ´å…³é”®ä½ç½®
          - é‡ä»·ç‰¹å¾ï¼šæ”¾é‡ä¸Šæ¶¨/ç¼©é‡è°ƒæ•´/é‡ä»·èƒŒç¦»
          - å½¢æ€ç‰¹å¾ï¼šå¤´è‚©åº•ã€åŒåº•ã€ä¸‰è§’å½¢æ•´ç†ç­‰

          ### æ“ä½œå»ºè®®
          æä¾›å…·ä½“çš„äº¤æ˜“ç­–ç•¥ï¼š
          - ç†æƒ³ä¹°ç‚¹ï¼šå¦‚"å›è°ƒè‡³å‰æœŸæ”¯æ’‘ä½é™„è¿‘"
          - åŠ ä»“æ¡ä»¶ï¼šå¦‚"æ”¾é‡çªç ´XXå…³é”®ä½ç½®"
          - æ­¢æŸå‚è€ƒï¼šå¦‚"è·Œç ´XXé‡è¦æ”¯æ’‘"
          - æŒä»“å‘¨æœŸï¼šçŸ­çº¿ï¼ˆ1-3å¤©ï¼‰/ä¸­çº¿ï¼ˆ1-2å‘¨ï¼‰/é•¿çº¿ï¼ˆ1æœˆ+ï¼‰

          ## 4. è¾“å‡ºè¦æ±‚
          - å½¢æˆ1500å­—ä»¥å†…çš„ä¸“ä¸šåˆ†ææŠ¥å‘Š
          - ç»“æ„æ¸…æ™°ï¼šæ‘˜è¦â†’çƒ­ç‚¹åˆ†æâ†’ä¸ªè‚¡èšç„¦â†’æ“ä½œç­–ç•¥
          - è¯­è¨€é£æ ¼ï¼šä¸“ä¸šã€ç®€æ´ã€å®¢è§‚
          - ç›®æ ‡è¯»è€…ï¼šæœºæ„æŠ•èµ„è€…å’Œä¸“ä¸šäº¤æ˜“å‘˜

          ## 5. ä»Šæ—¥é‡ç‚¹å…³æ³¨
          åˆ—å‡º3-5ä¸ªæœ€å€¼å¾—å…³æ³¨çš„æ ‡çš„ï¼Œæ¯é¡¹åŒ…å«ï¼š
          - æ ‡çš„åç§°åŠä»£ç 
          - å…³æ³¨ç†ç”±ï¼ˆä¸è¶…è¿‡20å­—ï¼‰
          - å…³é”®ä»·ä½æç¤ºï¼ˆå¦‚"å…³æ³¨çªç ´æœºä¼š"ã€"å›è°ƒä¼ç¨³æœºä¼š"ï¼‰
          - é£é™©ç­‰çº§ï¼šé«˜/ä¸­/ä½

          # é‡è¦æ³¨æ„äº‹é¡¹
          1. ä¸¥ç¦ç¼–é€ æˆ–çŒœæµ‹å…·ä½“è‚¡ä»·æ•°æ®
          2. æ‰€æœ‰åˆ†æå¿…é¡»åŸºäºæ–°é—»æ–‡æœ¬æä¾›çš„ä¿¡æ¯
          3. æŠ€æœ¯åˆ†æè¦æ³¨æ˜æ˜¯åŸºäºæ–‡æœ¬ä¸­æåˆ°çš„æŠ€æœ¯ä¿¡å·
          4. æ˜ç¡®æ ‡æ³¨"å¸‚åœºæœ‰é£é™©ï¼ŒæŠ•èµ„éœ€è°¨æ…"
          5. ä¿æŒå®¢è§‚ä¸­ç«‹ï¼Œé¿å…è¿‡åº¦ä¹è§‚æˆ–æ‚²è§‚è¡¨è¿°
             """},
            {"role": "user", "content": text}
        ]
    )
    return completion.choices[0].message.content.strip()

# å‘é€å¾®ä¿¡æ¨é€
def send_to_wechat(title, content):
    for key in server_chan_keys:
        url = f"https://sctapi.ftqq.com/{key}.send"
        data = {"title": title, "desp": content}
        response = requests.post(url, data=data, timeout=10)
        if response.ok:
            print(f"âœ… æ¨é€æˆåŠŸ: {key}")
        else:
            print(f"âŒ æ¨é€å¤±è´¥: {key}, å“åº”ï¼š{response.text}")


if __name__ == "__main__":
    today_str = today_date().strftime("%Y-%m-%d")

    # æ¯ä¸ªç½‘ç«™è·å–æœ€å¤š 5 ç¯‡æ–‡ç« ï¼ˆå¯Œé€”ç‰›ç‰›ã€æ ¼éš†æ±‡ã€æ™ºé€šè´¢ç»å’Œåå°”è¡—è§é—»é™¤å¤–ï¼‰
    articles_data, analysis_text = fetch_rss_articles(rss_feeds, max_articles=7)

    # AIç”Ÿæˆæ‘˜è¦
    summary = summarize(analysis_text)

    # ç”Ÿæˆä»…å±•ç¤ºæ ‡é¢˜å’Œé“¾æ¥çš„æœ€ç»ˆæ¶ˆæ¯
    final_summary = f"ğŸ“… **{today_str} è´¢ç»æ–°é—»æ‘˜è¦**\n\nâœï¸ **ä»Šæ—¥åˆ†ææ€»ç»“ï¼š**\n{summary}\n\n---\n\n"
    for category, content in articles_data.items():
        if content.strip():
            final_summary += f"## {category}\n{content}\n\n"

    # æ¨é€åˆ°å¤šä¸ªserveré…±key
    send_to_wechat(title=f"ğŸ“Œ {today_str} è´¢ç»æ–°é—»æ‘˜è¦", content=final_summary)
